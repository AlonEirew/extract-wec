# Extract WEC Dataset
This project is following our research paper: <a href="https://arxiv.org/abs/2104.05022">״WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia״</a><br/>
Here can be found both The WEC-Eng cross document dataset from English Wikipedia and the method for creating WEC for other languages. <br/><br/>
*Note: In our original WEC paper, we used several methods that were all aggregated into one project here. To that end, we replaced some of the original python implementations with corollating Java ones (for examle: SpaCy implementation replaced with StanfordNLP).*<br/>

### WEC-Eng Coreference Dataset
- Datset location is at `WEC/WEC-Eng.zip` file, split to dev/test/train 
- Dev & Test sets were manually validated 
- Also, the unfiltered version of WEC (without lexical control and un-partitioned to train/dev/test) is available in this same folder (`WEC-Unfiltered.zip`)

## Generating a new WEC Dataset
Below are the instructions of how-to generate a new version of WEC, whether required from a more recent English Wikipdia dump, 
or in order to extract it from one of the other supported languages (e.g., French, Spanish, German, Chinese). 
### Requisites
* A Wikipedia ElasticSearch Index created by <a href="https://github.com/AlonEirew/wikipedia-to-elastic">wikipedia-to-elastic</a>
  project (index must contain at least the <a href="https://github.com/AlonEirew/wikipedia-to-elastic#project-configuration-files">Infobox</a> "relationTypes").
* Java 11

### Processes
This code repo contains two main processes:
1) Code to generate the initial crude version of WEC-Lang
2) Code to generate the final Json of WEC-Lang

### WEC to DB Configuration:
Configuration file - `resources/application.properties`
```
spring.datasource.url=jdbc:h2:file:/demo => h2 database file url
poolSize=8 => Number of thread to run
elasticHost=localhost => Elastic engine host
elasticPort=9200 => (Elastic engine port)
elasticWikiIndex=enwiki_v3 => (Elastic index to read from (as generated by *wikipedia-to-elastic*)
infoboxConfiguration=/infobox_config/en_infobox_config.json => Explained below
multiRequestInterval=100 (recommended value) => Control the number of search pages to retrive from elastic
elasticSearchInterval=100 (recommended value) => Control the number of pages to read by the elasitc scroller
totalAmountToExtract=-1 => if < 0 then read all wikipedia pages, otherwise will read upto the amount specified
```

### WEC to Json Configuration:
```
main.lexicalThresh=4 => lexical diversity threshold
main.outputDir=output => the output folder where WEC json should be created and saved 
main.outputFile=GenWEC.json => WEC json file name, will contain the final version of the generated dataset 
```

### Language Adaptation
We have extracted the relevant infobox configuration for the English Wikipedia. <br/> 
In order to create a newer version of WEC-Eng, use/update the default `infobox_config/en_infobox_config.json` in configuration. <br/>

To generate WEC in one of the supported languages (other than English) follow those steps:
* Export Wikipedia in the required language using <a href="https://github.com/AlonEirew/wikipedia-to-elastic">wikipedia-to-elastic</a> project
* Explore for infoboxs categories, the script below can help by producing candidate as well as the amount of pages related to an infobox category.<br/>
`#>java -Xmx90000m -DentityExpansionLimit=2147480000 -DtotalEntitySizeLimit=2147480000 -Djdk.xml.totalEntitySizeLimit=2147480000 -cp "lib/*" scripts.ExtractInfoboxs` Report will be generated to `output/InfoboxCount.txt`
* Create an infobox configuration (new language) file in `src/main/resources/infobox_config/<lang>_infobox_config.json` <br/> 
File should contain all needed infobox language specific configurations. 
* Finally, set it as the `infoboxConfiguration` file in `application.properties`<br/>

#### English infobox example (from - `en_infobox_config.json`)
```
{
  "infoboxLangText" : "Infobox", // wikipedia markdown element name in the language (e.g., <Infobox sport>)
  "infoboxConfigs": [
    {
      "corefType": "ACCIDENT_EVENT", // Type you would like to give the infobox category
      "include": true, // Should be included when extracting WEC
      "infoboxs": [ // list of infobox categories that should be included in this type (lowercased and concat)
        "airlinerincident", 
        "airlineraccident",
        "aircraftcrash",
        "aircraftaccident",
        "aircraftincident",
        "aircraftoccurrence",
        "railaccident",
        "busaccident",
        "publictransitaccident"
      ]
    }
  ]
}
```

### Extracting WEC-Lang
Make sure the Wikipedia Elastic engine is running <br/>
* Running WikiToWECMain in order to generate the H2 database:<br/>
  `#>./gradlew bootRun --args=wecdb` <br/>
  **Program output** - an H2 dataset containing the crude extraction of coreference relations from Wikipedia (this resource can be used for experiments before generating the final version of WEC-Lang)
* Generate the WEC-Lang Json format file:<br/> 
  `#>./gradlew bootRun --args=wecjson` <br/>
  **Program output** - A JSON format resource of the WEC-Lang dataset
  

#### Visualization And Stats
In order to produce more statistics and/or create a visualized output of the generated dataset, refer to 
<a href="https://github.com/AlonEirew/cross-doc-event-coref#additional-scripts-helper_scripts">those scripts</a> for more information.
